{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import pymongo\n",
    "from pprint import pprint as pp\n",
    "import csv\n",
    "from collections import namedtuple\n",
    "import time\n",
    "\n",
    "import twitter\n",
    "import urllib.parse\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class IO_json(object):\n",
    "    def __init__(self, filepath, filename, filesuffix='json'):\n",
    "        self.filepath = filepath\n",
    "        self.filename = filename\n",
    "        self.filesuffix = filesuffix\n",
    "        \n",
    "    def save(self, data):\n",
    "        if os.path.isfile('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix)):\n",
    "            with io.open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'a', encoding='utf-8') as f:\n",
    "                f.write(json.dumps(data, ensure_ascii=False))\n",
    "                \n",
    "        else:\n",
    "            with io.open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'w', encoding='utf-8') as f:\n",
    "                f.write(json.dumps(data, ensure_ascii=False))\n",
    "                \n",
    "    def load(self):\n",
    "        with io.open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "        \n",
    "\n",
    "class IO_csv(object):\n",
    "    def __init__(self, filepath, filename, filesuffix='csv'):\n",
    "        self.filepath=filepath\n",
    "        self.filename=filename\n",
    "        self.filesuffix = filesuffix\n",
    "        \n",
    "    def save(self, data, NTname, fields):\n",
    "        NTuple = namedtuple(NTname, fields)\n",
    "        \n",
    "        if os.path.isfile('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix)):\n",
    "            with open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'ab') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerows([row for row in map(NTuple._make,data)])\n",
    "        else:\n",
    "            with open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'w', encoding='utf-8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(fields)\n",
    "                writer.writerows([row for row in map(NTuple._make, data)])\n",
    "                \n",
    "    def load(self, NTname, fields):\n",
    "        NTuple = namedtuple(NTname, fields)\n",
    "        with open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix),'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in map(NTuple._make, reader):\n",
    "                yield row\n",
    "                \n",
    "def parse_date(s):\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(s,'%a %b %d %H:%M:%S +0000 %Y'))\n",
    "\n",
    "def parse_geo(g,index):\n",
    "    try:\n",
    "        return str(g[\"geo\"][\"coordinates\"][index])\n",
    "    except:\n",
    "        return \"\"\n",
    "    \n",
    "def extract_tweet(statuses):\n",
    "    return [ {'id':status['id'], \n",
    "              'created_at':parse_date(status['created_at']), \n",
    "              'user_id':status['user']['id'], \n",
    "              'user_name':status['user']['name'],\n",
    "              'tweet_text':status['text'].encode('utf-8'),\n",
    "              'url':url['expanded_url']}\n",
    "                    for status in statuses\n",
    "                        for url in status['entities']['urls']\n",
    "           ]\n",
    "\n",
    "def extract_tweet_noURL(statuses):\n",
    "    return [ {'id'         :status['id'], \n",
    "              'created_at' :parse_date(status['created_at']), \n",
    "              'user_id'    :status['user']['id'],\n",
    "              'user_name'  :status['user']['name'], \n",
    "              'tweet_text' :status['text'].encode('utf-8') }\n",
    "                               for status in statuses ]\n",
    "\n",
    "fields01 = ['id','created_at','user_id','user_name','tweet_text','url']\n",
    "Tweet01 = namedtuple('Tweet01', fields01)\n",
    "\n",
    "def parse_tweet(data):\n",
    "    return Tweet01(\n",
    "        id=data.get('id',None),\n",
    "        created_at=data.get('created_at', None),\n",
    "        user_id=data.get('user_id', None),\n",
    "        user_name=data.get('user_name', None),\n",
    "        tweet_text=data.get('tweet_text', None),\n",
    "        url=data.get('url')\n",
    "    )\n",
    "\n",
    "fields00 = ['id', 'created_at', 'user_id', 'user_name', 'tweet_text']\n",
    "Tweet00 = namedtuple('Tweet00', fields00)\n",
    "\n",
    "def parse_tweet_noURL(data):\n",
    "    return Tweet00(\n",
    "        id=data.get('id',None),\n",
    "        created_at=data.get('created_at',None),\n",
    "        user_id=data.get('user_id',None),\n",
    "        user_name=data.get('user_name',None),\n",
    "        tweet_text=data.get('tweet_text',None)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "class TwitterAPI(object):\n",
    "    def __init__(self):\n",
    "        consumer_key = 'bxZ2ypZgrxusqKvUGnlv1KNew'\n",
    "        consumer_secret = 'HiAk5tuYc657MNKSb8odAxnU33fm4nLAAXY10PkhM3uUxbP9XB'\n",
    "        access_token = '2983449373-pH31LSzmmrbgXrkZx08FWDnWqxNmWx74aMwLMt4'\n",
    "        access_secret = 'Q41AgD2NdZ6n2UQWh8YM6nMosCKLENWrfqh1oiUNhp4R9'\n",
    "        self.consumer_key = consumer_key\n",
    "        self.consumer_secret = consumer_secret\n",
    "        self.access_token = access_token\n",
    "        self.access_secret = access_secret\n",
    "        self.retries = 3\n",
    "        self.auth = twitter.oauth.OAuth(access_token, access_secret, consumer_key, consumer_secret)\n",
    "        self.api = twitter.Twitter(auth=self.auth)\n",
    "        \n",
    "        appName = 'twt150530'\n",
    "        self.logger = logging.getLogger(appName)\n",
    "        logPath = './'\n",
    "        fileName = appName\n",
    "        fileHandler = logging.FileHandler(\"{0}/{1}.log\".format(logPath, fileName))\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        fileHandler.setFormatter(formatter)\n",
    "        self.logger.addHandler(fileHandler)\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "        \n",
    "        jsonFpath = './'\n",
    "        jsonFname = 'twtr15053001'\n",
    "        self.jsonSaver = IO_json(jsonFpath, jsonFname)\n",
    "        \n",
    "        #self.mongoSaver = IO_mongo(db='twtr01_db', coll='twtr01_coll')\n",
    "    \n",
    "    def searchTwitter(self, q, max_res=10, **kwargs):\n",
    "        search_results = self.api.search.tweets(q=q, count=20, **kwargs)\n",
    "        statuses = search_results['statuses']\n",
    "        max_results = min(1000, max_res)\n",
    "        \n",
    "        for _ in range(10):\n",
    "            try:\n",
    "                next_results = search_results['search_metadata']['next_results']\n",
    "            except KeyError as e:\n",
    "                self.logger.error('error in searchTwitter: %s', e)\n",
    "                break\n",
    "                \n",
    "            next_results = urllib.parse.parse_qsl(next_results[1:])\n",
    "            kwargs = dict(next_results)\n",
    "            search_results = self.api.search.tweets(**kwargs)\n",
    "            statuses += search_results['statuses']\n",
    "            self.saveTweets(search_results['statuses'])\n",
    "                \n",
    "            if len(statuses) > max_results:\n",
    "                self.logger.info('info in searchTwitter - got %i tweets - max: %i' %(len(statuses), max_results))\n",
    "                break\n",
    "        \n",
    "        return statuses\n",
    "    \n",
    "    def saveTweets(self, statuses):\n",
    "        self.jsonSaver.save(statuses)\n",
    "        \n",
    "        #for s in statuses:\n",
    "        #    self.mongoSaver.save(s)\n",
    "            \n",
    "    def parseTweets(self, statuses):\n",
    "        return [ (status['id'],\n",
    "                  status['created_at'],\n",
    "                  status['user']['id'],\n",
    "                  status['user']['name'],\n",
    "                  status['text'],\n",
    "                  url['expanded_url'])\n",
    "                        for status in statuses\n",
    "                            for url in status['entities']['urls']\n",
    "                 ]\n",
    "    \n",
    "    def getTweets(self, q, max_res=10):\n",
    "        def handleError(e, wait_period=2, sleep_when_rate_limited=True):\n",
    "            if wait_period > 3600:\n",
    "                self.logger.error('Too many retries in getTweets: %s', e)\n",
    "                raise e\n",
    "            if e.e.code == 401:\n",
    "                self.logger.error('error 401 * Not Authorized * in getTweets: %s', e)\n",
    "                return None\n",
    "            elif e.e.code == 404:\n",
    "                self.logger.error('error 404 * Not Found * in getTweets: %s', e)\n",
    "                return None\n",
    "            elif e.e.code == 429:\n",
    "                self.logger.error('error 429 * API Rate Limit Exceeded * in getTweets: %s', e)\n",
    "                if sleep_when_rate_limited:\n",
    "                    self.logger.error('error 429 * Retrying in 15 minutes * in getTweets: %s', e)\n",
    "                    sys.stderr.flush()\n",
    "                    time.sleep(60*15 + 5)\n",
    "                    self.logger.info('error 429 * Retrying now * in getTweets: %s', e)\n",
    "                    return 2\n",
    "                else:\n",
    "                    raise e\n",
    "            elif e.e.code in (500, 502, 503, 504):\n",
    "                self.logger.info('Encountered %i Error. Retrying in %i seconds' % (e.e.code, wait_period))\n",
    "                time.sleep(wait_period)\n",
    "                wait_period *= 1.5\n",
    "                return wait_period\n",
    "            else:\n",
    "                self.logger.error('Exit - aborting - %s', e)\n",
    "                raise e\n",
    "                \n",
    "        while True:\n",
    "            try:\n",
    "                self.searchTwitter(q, max_res=10)\n",
    "            except twitter.api.TwitterHTTPError as e:\n",
    "                error_count=0\n",
    "                wait_period = handleError(e, wait_period)\n",
    "                if wait_period is None:\n",
    "                    return\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = TwitterAPI()\n",
    "q = 'ApacheSpark'\n",
    "tsearch = t.searchTwitter(q)\n",
    "#t.saveTweets(tsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jsonFpath = './'\n",
    "jsonFname = 'twtr15053001'\n",
    "\n",
    "twts_ld = IO_json(jsonFpath, jsonFname).load()\n",
    "twts_js = json.loads(twts_ld)\n",
    "\n",
    "twts_ls_no_url = extract_tweet_noURL(twts_js)\n",
    "twts_ls_url = extract_tweet(twts_js)\n",
    "\n",
    "twts_nt_no_url = [parse_tweet_noURL(t) for t in twts_ls_no_url]\n",
    "twts_nt_url = [parse_tweet(t) for t in twts_ls_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvFpath = './'\n",
    "csvFname = 'twtr15051401'\n",
    "csvSuffix = 'csv'\n",
    "twts_csv = IO_csv(csvFpath, csvFname, csvSuffix)\n",
    "\n",
    "fields = ['id', 'created_at', 'user_id', 'user_name', 'tweet_text', 'url']\n",
    "Tweet_NT = 'Tweet01'\n",
    "twts_csv.save(twts_nt_url, Tweet_NT, fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Nikkimehta/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:58: DeprecationWarning: 'U' mode is deprecated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twts_csv_read = [t for t in twts_csv.load(Tweet_NT, fields)]\n",
    "len(twts_csv_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>858036595391250432</td>\n",
       "      <td>2017-04-28 19:14:00</td>\n",
       "      <td>230403822</td>\n",
       "      <td>Digital Distribution</td>\n",
       "      <td>b'RT @kdnuggets: Learning PySpark - Jump Start...</td>\n",
       "      <td>http://buff.ly/2oQpAIY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id           created_at    user_id  \\\n",
       "count                   10                   10         10   \n",
       "unique                   9                    9          8   \n",
       "top     858036595391250432  2017-04-28 19:14:00  230403822   \n",
       "freq                     2                    2          2   \n",
       "\n",
       "                   user_name  \\\n",
       "count                     10   \n",
       "unique                     8   \n",
       "top     Digital Distribution   \n",
       "freq                       2   \n",
       "\n",
       "                                               tweet_text  \\\n",
       "count                                                  10   \n",
       "unique                                                  5   \n",
       "top     b'RT @kdnuggets: Learning PySpark - Jump Start...   \n",
       "freq                                                    5   \n",
       "\n",
       "                           url  \n",
       "count                       10  \n",
       "unique                       5  \n",
       "top     http://buff.ly/2oQpAIY  \n",
       "freq                         5  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from blaze import Data, by, join, merge\n",
    "from odo import odo\n",
    "\n",
    "twts_pd_df = pd.DataFrame(twts_csv_read, columns=Tweet01._fields)\n",
    "twts_pd_df = twts_pd_df.drop(twts_pd_df.index[:1])\n",
    "\n",
    "twts_pd_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class '__main__.Tweet01'>\n"
     ]
    }
   ],
   "source": [
    "pp(type(twts_csv_read))\n",
    "pp(type(twts_csv_read[1]))\n",
    "#twts_bz_df = Data(twts_csv_read, columns=Tweet01._fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
