{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import pymongo\n",
    "from pprint import pprint as pp\n",
    "import csv\n",
    "from collections import namedtuple\n",
    "import time\n",
    "\n",
    "import twitter\n",
    "import urllib.parse\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class IO_json(object):\n",
    "    def __init__(self, filepath, filename, filesuffix='json'):\n",
    "        self.filepath = filepath\n",
    "        self.filename = filename\n",
    "        self.filesuffix = filesuffix\n",
    "        \n",
    "    def save(self, data):\n",
    "        if os.path.isfile('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix)):\n",
    "            with io.open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'a', encoding='utf-8') as f:\n",
    "                f.write(json.dumps(data, ensure_ascii=False))\n",
    "                \n",
    "        else:\n",
    "            with io.open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'w', encoding='utf-8') as f:\n",
    "                f.write(json.dumps(data, ensure_ascii=False))\n",
    "                \n",
    "    def load(self):\n",
    "        with io.open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "        \n",
    "\n",
    "class IO_csv(object):\n",
    "    def __init__(self, filepath, filename, filesuffix='csv'):\n",
    "        self.filepath=filepath\n",
    "        self.filename=filename\n",
    "        self.filesuffix = filesuffix\n",
    "        \n",
    "    def save(self, data, NTname, fields):\n",
    "        NTuple = namedtuple(NTname, fields)\n",
    "        \n",
    "        if os.path.isfile('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix)):\n",
    "            with open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'ab') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerows([row for row in map(NTuple._make,data)])\n",
    "        else:\n",
    "            with open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'wb') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerows(fields)\n",
    "                writer.writerows([row for row in map(NTuple._make, data)])\n",
    "                \n",
    "    def load(self, NTname, fields):\n",
    "        NTuple = namedtuple(NTName, fields)\n",
    "        with open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix),'rU') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in map(NTuple._make, reader):\n",
    "                yield row\n",
    "                \n",
    "def parse_date(s):\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(s,'%a %b %d %H:%M:%S +0000 %Y'))\n",
    "\n",
    "def parse_geo(g,index):\n",
    "    try:\n",
    "        return str(g[\"geo\"][\"coordinates\"][index])\n",
    "    except:\n",
    "        return \"\"\n",
    "    \n",
    "def extract_tweet(statuses):\n",
    "    return [ {'id':status['id'], \n",
    "              'created_at':parse_date(status['created_at']), \n",
    "              'user_id':status['user']['id'], \n",
    "              'user_name':status['user']['name'],\n",
    "              'tweet_text':status['text'].encode('utf-8'),\n",
    "              'url':url['expanded_url']}\n",
    "                    for status in statuses\n",
    "                        for url in status['entities']['urls']\n",
    "           ]\n",
    "\n",
    "def extract_tweet_noURL(statuses):\n",
    "    return [ {'id'         :status['id'], \n",
    "              'created_at' :parse_date(status['created_at']), \n",
    "              'user_id'    :status['user']['id'],\n",
    "              'user_name'  :status['user']['name'], \n",
    "              'tweet_text' :status['text'].encode('utf-8') }\n",
    "                               for status in statuses ]\n",
    "\n",
    "fields01 = ['id','created_at','user_id','user_name','tweet_text','url']\n",
    "Tweet01 = namedtuple('Tweet01', fields01)\n",
    "\n",
    "def parse_tweet(data):\n",
    "    return Tweet01(\n",
    "        id=data.get('id',None),\n",
    "        created_at=data.get('created_at', None),\n",
    "        user_id=data.get('user_id', None),\n",
    "        user_name=data.get('user_name', None),\n",
    "        tweet_text=data.get('tweet_text', None),\n",
    "        url=data.get('url')\n",
    "    )\n",
    "\n",
    "fields00 = ['id', 'created_at', 'user_id', 'user_name', 'tweet_text']\n",
    "Tweet00 = namedtuple('Tweet00', fields00)\n",
    "\n",
    "def parse_tweet_noURL(data):\n",
    "    return Tweet00(\n",
    "        id=data.get('id',None),\n",
    "        created_at=data.get('created_at',None),\n",
    "        user_id=data.get('user_id',None),\n",
    "        user_name=data.get('user_name',None),\n",
    "        tweet_text=data.get('tweet_text',None)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "class TwitterAPI(object):\n",
    "    def __init__(self):\n",
    "        consumer_key = 'bxZ2ypZgrxusqKvUGnlv1KNew'\n",
    "        consumer_secret = 'HiAk5tuYc657MNKSb8odAxnU33fm4nLAAXY10PkhM3uUxbP9XB'\n",
    "        access_token = '2983449373-pH31LSzmmrbgXrkZx08FWDnWqxNmWx74aMwLMt4'\n",
    "        access_secret = 'Q41AgD2NdZ6n2UQWh8YM6nMosCKLENWrfqh1oiUNhp4R9'\n",
    "        self.consumer_key = consumer_key\n",
    "        self.consumer_secret = consumer_secret\n",
    "        self.access_token = access_token\n",
    "        self.access_secret = access_secret\n",
    "        self.retries = 3\n",
    "        self.auth = twitter.oauth.OAuth(access_token, access_secret, consumer_key, consumer_secret)\n",
    "        self.api = twitter.Twitter(auth=self.auth)\n",
    "        \n",
    "        appName = 'twt150530'\n",
    "        self.logger = logging.getLogger(appName)\n",
    "        logPath = './'\n",
    "        fileName = appName\n",
    "        fileHandler = logging.FileHandler(\"{0}/{1}.log\".format(logPath, fileName))\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        fileHandler.setFormatter(formatter)\n",
    "        self.logger.addHandler(fileHandler)\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "        \n",
    "        jsonFpath = './'\n",
    "        jsonFname = 'twtr15053001'\n",
    "        self.jsonSaver = IO_json(jsonFpath, jsonFname)\n",
    "        \n",
    "        #self.mongoSaver = IO_mongo(db='twtr01_db', coll='twtr01_coll')\n",
    "    \n",
    "    def searchTwitter(self, q, max_res=10, **kwargs):\n",
    "        search_results = self.api.search.tweets(q=q, count=10, **kwargs)\n",
    "        statuses = search_results['statuses']\n",
    "        max_results = min(1000, max_res)\n",
    "        \n",
    "        for _ in range(10):\n",
    "            try:\n",
    "                next_results = search_results['search_metadata']['next_results']\n",
    "            except KeyError as e:\n",
    "                self.logger.error('error in searchTwitter: %s', e)\n",
    "                break\n",
    "                \n",
    "            next_results = urllib.parse.parse_qsl(next_results[1:])\n",
    "            kwargs = dict(next_results)\n",
    "            search_results = self.api.search.tweets(**kwargs)\n",
    "            statuses += search_results['statuses']\n",
    "            self.saveTweets(search_results['statuses'])\n",
    "                \n",
    "            if len(statuses) > max_results:\n",
    "                self.logger.info('info in searchTwitter - got %i tweets - max: %i' %(len(statuses), max_results))\n",
    "                break\n",
    "        \n",
    "        return statuses\n",
    "    \n",
    "    def saveTweets(self, statuses):\n",
    "        self.jsonSaver.save(statuses)\n",
    "        \n",
    "        #for s in statuses:\n",
    "        #    self.mongoSaver.save(s)\n",
    "            \n",
    "    def parseTweets(self, statuses):\n",
    "        return [ (status['id'],\n",
    "                  status['created_at'],\n",
    "                  status['user']['id'],\n",
    "                  status['user']['name'],\n",
    "                  status['text'],\n",
    "                  url['expanded_url'])\n",
    "                        for status in statuses\n",
    "                            for url in status['entities']['urls']\n",
    "                 ]\n",
    "    \n",
    "    def getTweets(self, q, max_res=10):\n",
    "        def handleError(e, wait_period=2, sleep_when_rate_limited=True):\n",
    "            if wait_period > 3600:\n",
    "                self.logger.error('Too many retries in getTweets: %s', e)\n",
    "                raise e\n",
    "            if e.e.code == 401:\n",
    "                self.logger.error('error 401 * Not Authorized * in getTweets: %s', e)\n",
    "                return None\n",
    "            elif e.e.code == 404:\n",
    "                self.logger.error('error 404 * Not Found * in getTweets: %s', e)\n",
    "                return None\n",
    "            elif e.e.code == 429:\n",
    "                self.logger.error('error 429 * API Rate Limit Exceeded * in getTweets: %s', e)\n",
    "                if sleep_when_rate_limited:\n",
    "                    self.logger.error('error 429 * Retrying in 15 minutes * in getTweets: %s', e)\n",
    "                    sys.stderr.flush()\n",
    "                    time.sleep(60*15 + 5)\n",
    "                    self.logger.info('error 429 * Retrying now * in getTweets: %s', e)\n",
    "                    return 2\n",
    "                else:\n",
    "                    raise e\n",
    "            elif e.e.code in (500, 502, 503, 504):\n",
    "                self.logger.info('Encountered %i Error. Retrying in %i seconds' % (e.e.code, wait_period))\n",
    "                time.sleep(wait_period)\n",
    "                wait_period *= 1.5\n",
    "                return wait_period\n",
    "            else:\n",
    "                self.logger.error('Exit - aborting - %s', e)\n",
    "                raise e\n",
    "                \n",
    "        while True:\n",
    "            try:\n",
    "                self.searchTwitter(q, max_res=10)\n",
    "            except twitter.api.TwitterHTTPError as e:\n",
    "                error_count=0\n",
    "                wait_period = handleError(e, wait_period)\n",
    "                if wait_period is None:\n",
    "                    return\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = TwitterAPI()\n",
    "q = 'ApacheSpark'\n",
    "tsearch = t.searchTwitter(q)\n",
    "#t.saveTweets(tsearch)\n",
    "jsonFpath = './'\n",
    "jsonFname = 'twtr15053001'\n",
    "\n",
    "twts_ld = IO_json(jsonFpath, jsonFname).load()\n",
    "twts_js = json.loads(twts_ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'created_at': '2017-04-25 02:36:28',\n",
      "  'id': 856698394487779330,\n",
      "  'tweet_text': b'Running #BigDL, #DeepLearning for #ApacheSpark, on #AWS. #Bi'\n",
      "                b'gData #DeepLearning #MachineLearning #DataScience #AI https:'\n",
      "                b'//t.co/lf8GVEpvZw',\n",
      "  'user_id': 146725639,\n",
      "  'user_name': 'Sajeetharan'},\n",
      " {'created_at': '2017-04-25 01:26:56',\n",
      "  'id': 856680896052600832,\n",
      "  'tweet_text': b'RT @AnatolSokolenko: @ApacheSpark on @hadoop YARN #security '\n",
      "                b'Model Analysis https://t.co/ZzGk60v9lO',\n",
      "  'user_id': 2704548373,\n",
      "  'user_name': 'NoSQL'},\n",
      " {'created_at': '2017-04-25 01:13:49',\n",
      "  'id': 856677596108656640,\n",
      "  'tweet_text': b\"RT @Syncsort: .@ApacheSpark 2.0: A Look at What's New from @\"\n",
      "                b\"Cloudera's @SeanAndersonBD https://t.co/WQA3WKPLdq #Cloudera\"\n",
      "                b' #spark link to @ho\\xe2\\x80\\xa6',\n",
      "  'user_id': 92932166,\n",
      "  'user_name': 'Senthil Balakrishnan'},\n",
      " {'created_at': '2017-04-25 01:00:04',\n",
      "  'id': 856674136290189312,\n",
      "  'tweet_text': b'RT @demibenari: Gave another talk about #ApacheSpark 101 #Bi'\n",
      "                b'gData, great to Meetup with friends \\xe2\\x98\\xba\\xef'\n",
      "                b'\\xb8\\x8f\\nat #Taboola @taboola @ApacheSpark #NoSQL\\xe2'\n",
      "                b'\\x80\\xa6',\n",
      "  'user_id': 104520454,\n",
      "  'user_name': 'alejandro vergara'},\n",
      " {'created_at': '2017-04-25 00:59:39',\n",
      "  'id': 856674028890849280,\n",
      "  'tweet_text': b'RT @demibenari: Gave another talk about #ApacheSpark 101 #Bi'\n",
      "                b'gData, great to Meetup with friends \\xe2\\x98\\xba\\xef'\n",
      "                b'\\xb8\\x8f\\nat #Taboola @taboola @ApacheSpark #NoSQL\\xe2'\n",
      "                b'\\x80\\xa6',\n",
      "  'user_id': 846367747495919616,\n",
      "  'user_name': 'pybiandstuff'},\n",
      " {'created_at': '2017-04-25 00:58:55',\n",
      "  'id': 856673846614568961,\n",
      "  'tweet_text': b'RT @demibenari: Gave another talk about #ApacheSpark 101 #Bi'\n",
      "                b'gData, great to Meetup with friends \\xe2\\x98\\xba\\xef'\n",
      "                b'\\xb8\\x8f\\nat #Taboola @taboola @ApacheSpark #NoSQL\\xe2'\n",
      "                b'\\x80\\xa6',\n",
      "  'user_id': 2704548373,\n",
      "  'user_name': 'NoSQL'},\n",
      " {'created_at': '2017-04-25 00:38:08',\n",
      "  'id': 856668614447038466,\n",
      "  'tweet_text': b'RT @Infosourcer: \\xf0\\x9f\\x98\\x8d@ApacheSpark, @ApacheHive,@'\n",
      "                b'ApacheKudu?Want 2get involved in answering the \\xf0'\n",
      "                b\"\\x9f\\x97\\xba\\xef\\xb8\\x8f's biggest\\xe2\\x81\\x89\\xef\\xb8\\x8fLe\"\n",
      "                b\"t's chat!https://t.co/455iEsRgS\\xe2\\x80\\xa6\",\n",
      "  'user_id': 104520454,\n",
      "  'user_name': 'alejandro vergara'},\n",
      " {'created_at': '2017-04-25 00:37:20',\n",
      "  'id': 856668411413356549,\n",
      "  'tweet_text': b'\\xf0\\x9f\\x98\\x8d@ApacheSpark, @ApacheHive,@ApacheKudu?Want 2'\n",
      "                b\"get involved in answering the \\xf0\\x9f\\x97\\xba\\xef\\xb8\\x8f's \"\n",
      "                b\"biggest\\xe2\\x81\\x89\\xef\\xb8\\x8fLet's chat!\\xe2\\x80\\xa6 http\"\n",
      "                b's://t.co/Dv0UuUV9Yv',\n",
      "  'user_id': 1957211,\n",
      "  'user_name': 'Suzy Tonini'},\n",
      " {'created_at': '2017-04-24 23:19:24',\n",
      "  'id': 856648799573430272,\n",
      "  'tweet_text': b'RT @DataFlairWS: Run-Time #ApacheSpark Architecture https://'\n",
      "                b't.co/KN3ysAWpbP #BigData #Spark #Hadoop #Tutorial https://t.'\n",
      "                b'co/Mjh6FjRRXQ',\n",
      "  'user_id': 104520454,\n",
      "  'user_name': 'alejandro vergara'},\n",
      " {'created_at': '2017-04-24 23:19:07',\n",
      "  'id': 856648730921111552,\n",
      "  'tweet_text': b'RT @DataFlairWS: Run-Time #ApacheSpark Architecture https://'\n",
      "                b't.co/KN3ysAWpbP #BigData #Spark #Hadoop #Tutorial https://t.'\n",
      "                b'co/Mjh6FjRRXQ',\n",
      "  'user_id': 841636869838385152,\n",
      "  'user_name': 'Michael Banach'}]\n"
     ]
    }
   ],
   "source": [
    "twts_ls_no_url = extract_tweet_noURL(twts_js)\n",
    "pp(twts_ls_no_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'created_at': '2017-04-25 02:36:28',\n",
      "  'id': 856698394487779330,\n",
      "  'tweet_text': b'Running #BigDL, #DeepLearning for #ApacheSpark, on #AWS. #Bi'\n",
      "                b'gData #DeepLearning #MachineLearning #DataScience #AI https:'\n",
      "                b'//t.co/lf8GVEpvZw',\n",
      "  'url': 'https://aws.amazon.com/blogs/ai/?utm_content=buffer01731&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer',\n",
      "  'user_id': 146725639,\n",
      "  'user_name': 'Sajeetharan'},\n",
      " {'created_at': '2017-04-25 01:26:56',\n",
      "  'id': 856680896052600832,\n",
      "  'tweet_text': b'RT @AnatolSokolenko: @ApacheSpark on @hadoop YARN #security '\n",
      "                b'Model Analysis https://t.co/ZzGk60v9lO',\n",
      "  'url': 'http://engineering.rallyhealth.com/bigdata/security/2017/04/15/apache-spark-on-yarn-security-model-analysis.html',\n",
      "  'user_id': 2704548373,\n",
      "  'user_name': 'NoSQL'},\n",
      " {'created_at': '2017-04-25 01:13:49',\n",
      "  'id': 856677596108656640,\n",
      "  'tweet_text': b\"RT @Syncsort: .@ApacheSpark 2.0: A Look at What's New from @\"\n",
      "                b\"Cloudera's @SeanAndersonBD https://t.co/WQA3WKPLdq #Cloudera\"\n",
      "                b' #spark link to @ho\\xe2\\x80\\xa6',\n",
      "  'url': 'http://sync.st/2oRxdSt',\n",
      "  'user_id': 92932166,\n",
      "  'user_name': 'Senthil Balakrishnan'},\n",
      " {'created_at': '2017-04-25 00:37:20',\n",
      "  'id': 856668411413356549,\n",
      "  'tweet_text': b'\\xf0\\x9f\\x98\\x8d@ApacheSpark, @ApacheHive,@ApacheKudu?Want 2'\n",
      "                b\"get involved in answering the \\xf0\\x9f\\x97\\xba\\xef\\xb8\\x8f's \"\n",
      "                b\"biggest\\xe2\\x81\\x89\\xef\\xb8\\x8fLet's chat!\\xe2\\x80\\xa6 http\"\n",
      "                b's://t.co/Dv0UuUV9Yv',\n",
      "  'url': 'https://twitter.com/i/web/status/856668411413356549',\n",
      "  'user_id': 1957211,\n",
      "  'user_name': 'Suzy Tonini'},\n",
      " {'created_at': '2017-04-24 23:19:24',\n",
      "  'id': 856648799573430272,\n",
      "  'tweet_text': b'RT @DataFlairWS: Run-Time #ApacheSpark Architecture https://'\n",
      "                b't.co/KN3ysAWpbP #BigData #Spark #Hadoop #Tutorial https://t.'\n",
      "                b'co/Mjh6FjRRXQ',\n",
      "  'url': 'https://goo.gl/wFi3R5',\n",
      "  'user_id': 104520454,\n",
      "  'user_name': 'alejandro vergara'},\n",
      " {'created_at': '2017-04-24 23:19:07',\n",
      "  'id': 856648730921111552,\n",
      "  'tweet_text': b'RT @DataFlairWS: Run-Time #ApacheSpark Architecture https://'\n",
      "                b't.co/KN3ysAWpbP #BigData #Spark #Hadoop #Tutorial https://t.'\n",
      "                b'co/Mjh6FjRRXQ',\n",
      "  'url': 'https://goo.gl/wFi3R5',\n",
      "  'user_id': 841636869838385152,\n",
      "  'user_name': 'Michael Banach'}]\n"
     ]
    }
   ],
   "source": [
    "twts_ls_url = extract_tweet(twts_js)\n",
    "pp(twts_ls_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet00(id=856680896052600832, created_at='2017-04-25 01:26:56', user_id=2704548373, user_name='NoSQL', tweet_text=b'RT @AnatolSokolenko: @ApacheSpark on @hadoop YARN #security Model Analysis https://t.co/ZzGk60v9lO')\n"
     ]
    }
   ],
   "source": [
    "twts_nt_no_url = [parse_tweet_noURL(t) for t in twts_ls_no_url]\n",
    "print(twts_nt_no_url[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet01(id=856680896052600832, created_at='2017-04-25 01:26:56', user_id=2704548373, user_name='NoSQL', tweet_text=b'RT @AnatolSokolenko: @ApacheSpark on @hadoop YARN #security Model Analysis https://t.co/ZzGk60v9lO', url='http://engineering.rallyhealth.com/bigdata/security/2017/04/15/apache-spark-on-yarn-security-model-analysis.html')\n"
     ]
    }
   ],
   "source": [
    "twts_nt_url = [parse_tweet(t) for t in twts_ls_url]\n",
    "print(twts_nt_url[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0c53e82de78e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mTweet_NT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Tweet01'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtwts_csv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwts_nt_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTweet_NT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-c18fe4fe367c>\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, data, NTname, fields)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{0}/{1}.{2}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilesuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNTuple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "csvFpath = './'\n",
    "csvFname = 'twtr15051401'\n",
    "csvSuffix = 'csv'\n",
    "\n",
    "twts_csv = IO_csv(csvFpath, csvFname, csvSuffix)\n",
    "fields = ['id', 'created_at', 'user_id', 'user_name', 'tweet_text', 'url']\n",
    "Tweet_NT = 'Tweet01'\n",
    "\n",
    "twts_csv.save(twts_nt_url, Tweet_NT, fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
